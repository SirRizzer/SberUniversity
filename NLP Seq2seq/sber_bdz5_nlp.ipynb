{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSlmzb2uQHrX"
   },
   "source": [
    "# Домашняя работа: Обучаем собственную Seq2seq модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PEYCjfwQHrc"
   },
   "source": [
    "#### Установим зависимости.\n",
    "\n",
    "В конце вы получите сообщение \"You must restart the runtime in order to use newly installed versions.\". Нажмите кнопку \"RESTART RUNTIME\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fr9Flx-CQHrf",
    "outputId": "d3f58ecc-8b33-43de-b22c-731dd1f7ef68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.11.0 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (4.11.0)\n",
      "Requirement already satisfied: torch==1.10.2 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: torchtext==0.11.2 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (1.21.5)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (0.0.53)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (0.11.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from transformers==4.11.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from torch==1.10.2) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.11.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.11.0) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from requests->transformers==4.11.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from requests->transformers==4.11.0) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from requests->transformers==4.11.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from requests->transformers==4.11.0) (3.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.11.0) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.11.0) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\pyora\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.11.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Установим зависимости\n",
    "!pip install transformers==4.11.0 torch==1.10.2 torchtext==0.11.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX1s-VUYQHri"
   },
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOBmbnc6QHrj"
   },
   "source": [
    "Будем использовать англоязычный [датасет PennTreebank](https://pytorch.org/text/stable/datasets.html#penntreebank), доступный во фреймворке `torchtext`. \n",
    "\n",
    "Для начала заглянем в датасет и посмотрим, что представляет из себя пример из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s7CV_oreQHrl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2I7QDkUJQHrn",
    "outputId": "0121e4fd-9e74-4083-c70f-8a7e5f51f7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      "\n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      "\n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      "\n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \n",
      "\n",
      " a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import PennTreebank\n",
    "\n",
    "\n",
    "# создадим итератор по данным, загрузив готовый датасет PennTreebank из torchtext, \n",
    "# причем для начала, только обучающую train подвыборку, используя аргумент split\n",
    "train_iter = PennTreebank(split='train')\n",
    "# определим небольшое число примеров для демонстрации\n",
    "nsamples = 5\n",
    "# создадим переменную для сохранения текста примеров в единый текст\n",
    "# в дальнейшем мы его заиспользуем для проверки паайплайна предобработки текста\n",
    "training_samples_example = \"\"\n",
    "\n",
    "\n",
    "for line in train_iter:\n",
    "    print(line)\n",
    "    training_samples_example += line\n",
    "    nsamples -= 1\n",
    "    if nsamples == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "X2R9B4XlQHrt",
    "outputId": "43411334-d4ac-4124-e8aa-8d53afbc83c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n pierre <unk> N years old will join the board as a nonexecutive director nov. N \\n mr. <unk> is chairman of <unk> n.v. the dutch publishing group \\n rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \\n a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим, как выглядят эти примеры, соединенные в одну строку\n",
    "training_samples_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFCKojszQHsH"
   },
   "source": [
    "В данном тюториале мы собиаремся решать задачу sequence-to-sequence -- задачу предсказания следующего токена по предыдущим, то есть фактически задачу языкового моделирования. Поэтому нам не понадобятся пайплайны предобработки меток классов -- у нас их попросту нет. Поэтому далее мы переходим к предобработке текстов датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivqOU3QZQHsO"
   },
   "source": [
    "## Токенизация\n",
    "\n",
    "Для построения словаря, нам необходимо определить токенизатор данных -- модель, которая будетразделять единый текст на токены -- наиболее популярные единицы языка. \n",
    "\n",
    "В данной работе будем использовать базовый токенизатор для английского языка из библиотеки  `torchtext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhzTIscQHsU",
    "outputId": "ea06d196-1635-48c7-bf08-d6569af9c650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL SAMPLE:\n",
      " the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \n",
      "\n",
      "TOKENIZED SAMPLE:\n",
      "['the', 'asbestos', 'fiber', '<unk>', 'is', 'unusually', '<unk>', 'once', 'it', 'enters', 'the', '<unk>', 'with', 'even', 'brief', 'exposures', 'to', 'it', 'causing', 'symptoms', 'that', 'show', 'up', 'decades', 'later', 'researchers', 'said']\n",
      "ORIGINAL SAMPLE:\n",
      " <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \n",
      "\n",
      "TOKENIZED SAMPLE:\n",
      "['<unk>', 'inc', '.', 'the', 'unit', 'of', 'new', 'york-based', '<unk>', 'corp', '.', 'that', 'makes', 'kent', 'cigarettes', 'stopped', 'using', '<unk>', 'in', 'its', '<unk>', 'cigarette', 'filters', 'in', 'n']\n",
      "ORIGINAL SAMPLE:\n",
      " although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \n",
      "\n",
      "TOKENIZED SAMPLE:\n",
      "['although', 'preliminary', 'findings', 'were', 'reported', 'more', 'than', 'a', 'year', 'ago', 'the', 'latest', 'results', 'appear', 'in', 'today', \"'\", 's', 'new', 'england', 'journal', 'of', 'medicine', 'a', 'forum', 'likely', 'to', 'bring', 'new', 'attention', 'to', 'the', 'problem']\n",
      "ORIGINAL SAMPLE:\n",
      " a <unk> <unk> said this is an old story \n",
      "\n",
      "TOKENIZED SAMPLE:\n",
      "['a', '<unk>', '<unk>', 'said', 'this', 'is', 'an', 'old', 'story']\n",
      "ORIGINAL SAMPLE:\n",
      " we 're talking about years ago before anyone heard of asbestos having any questionable properties \n",
      "\n",
      "TOKENIZED SAMPLE:\n",
      "['we', \"'\", 're', 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "# загрузим базовый англоязычный basic_english токенизатор с помощью get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# определим небольшое число примеров для демонстрации\n",
    "nsamples = 5\n",
    "\n",
    "for line in train_iter:\n",
    "    # получим из строки line токены с помощью tokenizer\n",
    "    tokenized_line = tokenizer(line)\n",
    "    # распечатаем полученную информацию\n",
    "    print(f\"ORIGINAL SAMPLE:\\n{line}\")\n",
    "    print(f\"TOKENIZED SAMPLE:\\n{tokenized_line}\")\n",
    "    nsamples -= 1\n",
    "    if nsamples == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhTiy_OBQHsW"
   },
   "source": [
    "## Словарь\n",
    "\n",
    "Теперь нам необходимо собрать словарь -- структуру данных, содержащую проиндексированные токены, которые мы сможем использовать для векторизации текстов -- представлении текстов в удобном машиночитаемом формате. Для сбора словаря, так как нас интересуют непосредственно те слова, которые есть в нашем датасете (но при этом, мы не можем использовать слова из тестовой подвыборки датасета при создлании словаря), то будем строить словарь на основе итерирования по токенам по обучающей выборки.\n",
    "\n",
    "В данном тюториале мы попрбуем два разных способа создать словарь на основе итератора по текстам. Оба способа создадут **идентичные по содержащимся токенам словари**.\n",
    "\n",
    "**Первый способ** заключается в использование `Counter` для сбора токенов и инициализации  `torchtext.vocab.Vocab` на основе собранного `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOaQ23igQHsY",
    "outputId": "4995f8c8-be5e-42b0-cff1-7e534e45d7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocaublary size: 9922\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "# создадим итератор по данным, загрузив готовый датасет PennTreebank из torchtext, \n",
    "# причем для начала, только обучающую train подвыборку, используя аргумент split\n",
    "train_iter = PennTreebank(split='train')\n",
    "# создадим инстанс Counter\n",
    "counter = Counter()\n",
    "\n",
    "for line in train_iter:\n",
    "    # итерируясь по данным, токенизируем текст в каждом примере в датасете\n",
    "    tokens = tokenizer(line)\n",
    "    # добавим полученные токены в наш counter с помощью метода update\n",
    "    counter.update(tokens)\n",
    "\n",
    "# теперь создадим Vocab на основе нашего counter\n",
    "vocab = Vocab(counter)\n",
    "# посомтрим на размер собранного словаря\n",
    "print(f\"Vocaublary size: {vocab.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37zqqC7UQHsb"
   },
   "source": [
    "**Второй способ** состоит в создании словаря напрямую через итератор данных без использования `Counter`. Для этого воспользуемся функций `torchtext.vocab.build_vocab_from_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUKbEwU5QHsd",
    "outputId": "b548ccf6-daba-476f-9896-61b53d321917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocaublary size: 9922\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    # итерируясь по обучающему датасету\n",
    "    for line in data_iter:\n",
    "        # токенизируем line\n",
    "        tokens = tokenizer(line)\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "# создадим итератор по данным, загрузив готовый датасет PennTreebank из torchtext, \n",
    "# причем для начала, только обучающую train подвыборку, используя аргумент split\n",
    "train_iter = PennTreebank(split='train')\n",
    "# используем build_vocab_from_iterator для получения vocab на основе yield_tokens(),\n",
    "# обязательно добавив специальный токен \"<unk>\" в аргумент-список специальных токенов specials\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab['<unk>']) \n",
    "# посмотрим на размер собранного словаря\n",
    "print(f\"Vocaublary size: {vocab.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hTiCQLsQHsg"
   },
   "source": [
    "Как мы видим, словари имеют одинаковые размер. Оба способа создают идентичные по содержащимся токенам словари."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzsGXd9_QHsi"
   },
   "source": [
    "## Подготовим датасет\n",
    "\n",
    "Для подготовки датасета к процессу обучения модели seq2seq, необходимо \n",
    "* токенизировать тексты, \n",
    "* перевести токены в индексы токенов в словаре,\n",
    "* преобразовать результат в тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_IHj6SNHQHsk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    # подготовим переменную для индексов токенов\n",
    "    data = []\n",
    "    # итерируемся по данным\n",
    "    for item in raw_text_iter:\n",
    "        # каждый пример в датасете -- текст\n",
    "        # токенизируем полученный текст с помощью tokenizer\n",
    "        tokens = tokenizer(item)\n",
    "        # для каждого токена из полученных токенизатором\n",
    "        # получаем индекс токена в словаре с помощью vocab\n",
    "        # и получаем лист индексов токенов\n",
    "        tokens_ids = vocab(tokens)# ВАШ КОД ЗДЕСЬ\n",
    "        # преобразуем полученный лист индексов токенов текста \n",
    "        # в torch.tensor с типом dtype=torch.long\n",
    "        tensor_tokens_ids = torch.tensor(tokens_ids, dtype=torch.long)# ВАШ КОД ЗДЕСЬ\n",
    "        # добавим в список data полученный тензор, \n",
    "        # если наш сэмпл данных содержит непустые токены (такие примеры получаются,\n",
    "        # когда изначальная строка содержала только символы новой строки)\n",
    "        if len(tokens_ids) > 0:\n",
    "            data += [tensor_tokens_ids]\n",
    "    \n",
    "    return torch.cat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ryBoCnh9QHsk"
   },
   "outputs": [],
   "source": [
    "# а теперь перепишем эту функцию, но компактнее записанную\n",
    "# и использующую list comprehension вместо циклов for \n",
    "# (list comprehension работает быстрее)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) \n",
    "            for item in raw_text_iter if len(item.strip()) > 0]\n",
    "    return torch.cat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Cg8wTAaQHsm",
    "outputId": "afd34c79-5b7b-4901-cf57-582ccbd8aae0"
   },
   "outputs": [],
   "source": [
    "# создадим ТРИ итератора по данным, загрузив готовый датасет PennTreebank из torchtext\n",
    "train_iter, val_iter, test_iter = PennTreebank()\n",
    "\n",
    "# предобработаем данные обучающей выборки\n",
    "train_processed_data = data_process(train_iter)\n",
    "# предобработаем данные валидационной выборки\n",
    "val_processed_data = data_process(val_iter)# ВАШ КОД ЗДЕСЬ\n",
    "# предобработаем данные тестовой выборки\n",
    "test_processed_data = data_process(test_iter)# ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "re9wjvE8QHsr",
    "outputId": "210c3868-f5f1-4cb1-ff43-d776da317fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9892, 9893, 9894, 9896, 9897, 9898, 9902, 9903, 9904, 9905, 9906, 9908,\n",
       "        9909, 9910, 9911, 9913, 9914, 9915, 9916, 9917, 9918, 9919, 9920, 9921,\n",
       "        9187,    0,    2,   74,  395,   34, 2126,    1,  146,   20,    5, 9139,\n",
       "         275,  410,    8,    2,   24,    8,    0,   14,  141,    3,    0,    2,\n",
       "           8, 2506,    8,    1, 3070, 1595,   97, 7627,    0,    2,   74,  395,\n",
       "           7,  339,  141,    3, 2466,  659, 2162,  956,   25,  524,    5, 9139,\n",
       "         275,    3,   40,  303,  441, 3667,    5,  943,    3, 3137,  499,  263,\n",
       "           4,  138, 6053, 4223, 5997,   32,  988,    5,  241,  762,    3, 1016,\n",
       "        2778,  211,    5,   97])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NI56r1lmQHsv",
    "outputId": "50c13058-38d3-4089-bec6-859101d44949"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([924412])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# какой полуичлся размер обучающей выборки в числе токенов\n",
    "train_processed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgHLvtlgQHs0",
    "outputId": "5934c209-221f-4972-ba1a-a542722657a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# попробуем перевести индексы слов в словаре в токены с помощью метода lookup_tokens\n",
    "vocab.lookup_tokens(train_processed_data[:20].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UhOq8AOQHs3"
   },
   "source": [
    "Воспользуемся отложенной ранее строкой `training_samples_example`, чтобы проверить работу словаря и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQ3k1cTMQHs7",
    "outputId": "e32e2b49-1556-4bf9-f447-fb1cca580989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9892,\n",
       " 9893,\n",
       " 9894,\n",
       " 9896,\n",
       " 9897,\n",
       " 9898,\n",
       " 9902,\n",
       " 9903,\n",
       " 9904,\n",
       " 9905,\n",
       " 9906,\n",
       " 9908,\n",
       " 9909,\n",
       " 9910,\n",
       " 9911,\n",
       " 9913,\n",
       " 9914,\n",
       " 9915,\n",
       " 9916,\n",
       " 9917]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# перевести токены в индексы слов в словаре с помощью метода lookup_indices\n",
    "training_sample_indices = vocab.lookup_indices(tokenizer(training_samples_example)[:20])\n",
    "training_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYjqmgfKQHs9",
    "outputId": "48b1f9ac-2d95-420a-ad2c-354fd244d50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# перевести индексы слов в словаре в токены с помощью метода lookup_tokens\n",
    "vocab.lookup_tokens(training_sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-2PDeAeQHs9"
   },
   "source": [
    "Определим доступный нам для вычисления девайс: `cpu` или `gpu`. Для более эффективного и быстрого обучения, рекомендуется использовать `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPgMdV2zQHtA",
    "outputId": "9c2d3b06-4a95-4719-da3b-d9d883c952eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zRuasXiQHtD"
   },
   "source": [
    "## Батчевание данных\n",
    "\n",
    "Для задачи sequence-to-sequence также хочется использовать батчевание данных для обучения. Однако все примеры в данных обычно разной длины, что в некоторых задач, например, переводе текста или суммаризации, когда входная и целевая последовательности отличаются, приводит к тому, что для кжадого батча определяется собственная максимальная длина (либо длина самого длинного прмиера в батче, либо максимальная допустимая длина примера). В задаче языкового моделирования, а именно предсказания следующего токена последовательности, обычно используется другой способ батчевания данных -- все примеры батча склеиваются в единую последовательность, а затем разбиваются на батчи,а лишние элементы отбрасываются. Да, теряется связь между последовательными элементами, попавшивими в разные батчи, однако, это делает вычисления значительно эффективнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ThxlcOi1QHtD"
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    # Разделим размер данных data.size(0) на batch_size частей,\n",
    "    # чтобы получить целое число батчей\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # обрезаем все элементы, которые не помещаются в пространство nbatch * batch_size\n",
    "    # используем метод narrow для dim=0, start=0 и length=nbatch * batch_size\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # абсолютно аналогично работает следующая строка:\n",
    "    # data = data[:nbatch * batch_size]\n",
    "    \n",
    "    # теперь разделим данные на столбцы по размеру батча, используя метод view\n",
    "    data = data.view(batch_size, nbatch)\n",
    "    # транспонируем данные, используя метод t()\n",
    "    data = data.t()\n",
    "    # преобразуем наш тензор в непрерывный в памяти с помощью метода contiguous\n",
    "    data = data.contiguous()\n",
    "    # перенесем данные на девайс и вернем их\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "# определим размер батча на время обучения\n",
    "batch_size = 20\n",
    "# определим размер батч на время инференса\n",
    "eval_batch_size = 10\n",
    "\n",
    "# батчуем данные обучающей выборки с помощью нашей функции batchify\n",
    "train_data = batchify(train_processed_data, batch_size)\n",
    "# батчуем данные валидационной выборки с помощью нашей функции batchify\n",
    "val_data = batchify(val_processed_data, batch_size)\n",
    "# батчуем данные тестовой выборки с помощью нашей функции batchify\n",
    "test_data = batchify(test_processed_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUX12PKLQHtE",
    "outputId": "6d33be90-3fae-438d-aa7a-506d4aa166bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9892,    4,   31,   16, 1925,  178,    3, 2295,    5,   42, 3412, 1611,\n",
       "           1,  730, 3722, 5722, 2124, 1678,  796,  829])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqAVFNQcQHtJ",
    "outputId": "737954fd-8d0c-4c5a-9a7f-e5060f0eda9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46220, 20])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zui9G644QHtJ"
   },
   "source": [
    "Теперь мы видим, что у нас есть последовательности данных, неудобные для использования - первая размерность очень большая. Необходимо сделать итерации по первой размерности.\n",
    "\n",
    "\n",
    "Создадим функцию для получения определенного i-го батча данных.\n",
    "\n",
    "При этом введем переменную, содержащую максимальную длину последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VZ7nl0Q1QHtQ"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    Функция генерирует батч пар входная-выходная последовательности.\n",
    "    Так как последовательности являются длинными, \n",
    "    то мы используем не только размер батча, но и data chunks.\n",
    "    \n",
    "    Returns:\n",
    "    - data of dimension (bptt, batch_size)\n",
    "    - targets of dimension (bptt * batch_size)\n",
    "    \"\"\"\n",
    "    # определяем длину последовательности как минимум между bptt и\n",
    "    # длиной всех данных\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-2ybdYUQHtT",
    "outputId": "41672e77-6553-413b-ba98-2e9f69ff1e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n",
      "torch.Size([700])\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 1)\n",
    "print(data.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNSFOfiMQHtU"
   },
   "source": [
    "## Архитектура модели\n",
    "\n",
    "Данный тюториал предназначен для обучения модели Трансформер с нуля на задаче языкового моделирования. \n",
    "\n",
    "Напомним, что задача языкового моделирования заключается в предсказании распеределения вероятностей заданного токена встретиться сразу после заданной последовательности токенов, то есть по сути, предсказании следующего токена в последовательности.\n",
    "\n",
    "Для языкового моделирования можно использовать модели, состоящие только из энкодера (encoder-only), например модель BERT содержит только энкодер. \n",
    "\n",
    "Итак, будем использовать `torch.nn.TransformerEncoderLayer` и `torch.nn.TransformerEncoder` (который состоит из заданного количества `nn.TransformerEncoderLayer`) в качестве энкодера.\n",
    "\n",
    "Для создания такой модели необходима следующая последовательность действий:\n",
    "1. последовательность токенов (индексов токенов) передается в слой векторных представлений.\n",
    "2. далее последовательность передается в слой позиционных векторных представлений (positional encoding layer), отвечающих за передачу информации о последовательности токенов.\n",
    "3. Полученные векторные представления передаются в линейный слой, размерность которого соответствует размеру словаря. Данный слой предсказывает распределение вероятностей по словарю для следующего токена последовательности.\n",
    "\n",
    "Важной особенностью модели Трансформер является использование masked self-attention, данный метод не позволяет модели смотреть вперед (обращать внимание на токены, которые идут после заданного). Для этого используется специальная маска -- матрица, зануляющая элементы, на которые нельзя обращать внимание.\n",
    "\n",
    "\n",
    "На рисунке ниже представлена схема encoder-only модели. В качестве features  мы получаем скрытые векторные представления входной последовательности, которые пропускаются через линейный слой для получения распределения вероятностей по словарю для следующего токена. Причем таких Encoder block может быть расположено последовательно несколько штук.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwyDRwg5QHtX"
   },
   "source": [
    "## Позиционные векторные представления (Positional Encoding)\n",
    "\n",
    "Для передачи модели информации о взаимном расположении токенов необходимо также добавить позиционные векторные представления. Позиционные векторные представленияимеют те же размерности, что и векторные представления токенов самих по себе, что позволяет суммировать их в дальнейшем. В оригианльной работе Attention is All You Need авторы исследуют разные варианты используемых функций для позиционных векторных представлений и приходят к выводу, что используемые функции не играют значительной роли. Наиболее распространенными вариантами являются периодические функции синуса и косинуса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-z2tOFrDQHtZ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # создадим слой dropout как инстанс nn.Dropout слоя, \n",
    "        # получающего на вход значение dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # создадим переменную pe как нулевую матрицу torch.zeros размера (max_len x d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # создадим переменную position как вектор torch.arange типа torch.float размера max_len, \n",
    "        # содержащий последовательные значения от 0 до max_len-1\n",
    "        position = torch.arange(0, max_len, dtype = float)\n",
    "        # вытянем position вдоль оси 1 с помощью метода unsqueeze\n",
    "        position = torch.unsqueeze(position, 1)\n",
    "        # создадим переменную div_term как вектор torch.arange типа torch.float размера d_model / 2, \n",
    "        # содержащий каждое 2-е последовательное значение от 0 до d_model-1   \n",
    "        div_term = torch.arange(0, d_model, 2).float()\n",
    "        # домножим каждое значение div_term на специальный коэффициент (-math.log(10000.0) / d_model)\n",
    "        div_term = torch.exp(div_term * (-math.log(10000.0) / d_model))\n",
    "        # итак позиционные эмбеддинги нечетных элементов представляют из себя\n",
    "        # перемноженные position и div_term пропускают через sin\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # итак позиционные эмбеддинги четных элементов представляют из себя\n",
    "        # перемноженные position и div_term пропускают через cos\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # вытянем pe вдоль оси 0 с помощью метода unsqueeze\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # транспонируем ее с помощью метода transpose\n",
    "        pe = pe.transpose(0, 1)\n",
    "        # добавим pe переменную в буффер модуля\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # добавим в исходным веткорным представления pe соответствующего размера\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        # пропустим также выход через слой dropout\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPvsgqF6QHta"
   },
   "source": [
    "**Создадим саму архиектуру модели, использующую наши позиционные векторные представления**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kAsC9TncQHta"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, n_heads, hidden_dim, n_blocks, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # сохраним в атрибуты класса размер векторных представлений, \n",
    "        # чтобы он был доступен во всех методах класса\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        # зададим тип модели как 'Transformer'\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        # создадим слой pos_encoder как инстанс класса PositionalEncoding, \n",
    "        # принимающий на вход размер векторных представлений и дропаут\n",
    "        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n",
    "        \n",
    "        # создадим слой encoder_layers как инстанс слоя TransformerEncoderLayer, \n",
    "        # принимающий на вход размер векторных представлений, число голов, \n",
    "        # размер скрытого пространства и дропаут\n",
    "        encoder_layers = nn.TransformerEncoderLayer(emb_size, n_heads, hidden_dim, dropout)\n",
    "        \n",
    "        # создадим слой transformer_encoder как инстанс слоя TransformerEncoder, \n",
    "        # принимающий на вход на инстанс encoder_layers и желаемое число блоков энкодера\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_blocks)\n",
    "        \n",
    "        # создадим слой encoder как инстанс слоя nn.Embedding, \n",
    "        # принимающий на вход размер словаря и размер векторных представлений\n",
    "        self.encoder = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "        # создадим слой decoder как инстанс слоя nn.Linear, \n",
    "        # который преобразует вектора размерности векторных представлений в размер словаря\n",
    "        self.decoder = torch.nn.Linear(emb_size, vocab_size)\n",
    "        \n",
    "        # инциализируем веса нашей модели с помощью нашего метода init_weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        Данный метод создает квадратную маску размера (sz x sz) для self-attention.\n",
    "        \"\"\"\n",
    "        # создадим единичную матрицу torch.ones размера (sz x sz)\n",
    "        mask = torch.ones(sz, sz)\n",
    "        # превратим ее в верхнетреугольную матрицу с помощью torch.triu \n",
    "        # (элементы на диалогнали и выше диалогнали -- единицы, остальные -- нули)\n",
    "        mask = torch.triu(mask)\n",
    "        # транспонируем ее с помощью метода transpose\n",
    "        mask = mask.transpose(0, 1)\n",
    "        \n",
    "        # заменим значения, равные нулю, на -inf\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        # заменим значения, равные единице, на 0.0\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # инициализируем веса энкодера случайными величинами \n",
    "        # из равномерного распределения в диапазоне (-initrange, initrange) \n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # зададим в качестве начальных нулевые bias \n",
    "        self.decoder.bias.data.zero_()\n",
    "        # инициализируем веса декодера (линейного слоя) случайными величинами \n",
    "        # из равномерного распределения в диапазоне (-initrange, initrange) \n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # входная последовательность обрабатывается с помощью encoder\n",
    "        src = self.encoder(src)\n",
    "        # полученная последовательность домножается на квадратный корень \n",
    "        # из размера векторных представлений\n",
    "        src = src * math.sqrt(self.emb_size)\n",
    "        # полученная последовательность обрабатывается с помощью pos_encoder        \n",
    "        src = self.pos_encoder(src)\n",
    "        # полученная последовательность обрабатывается с помощью transformer_encoder,\n",
    "        # который также принимает на вход маску для self-attention\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        # полученная последовательность обрабатывается с помощью decoder \n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv04BKl2QHta"
   },
   "source": [
    "## Инициализация модели\n",
    "\n",
    "Теперь мы можем инициалировать класс модели. Выберем размер векторных представлений 200, но вы можете изменить на больший размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEoDhS7EQHta",
    "outputId": "7786e960-1d7c-40f3-b84f-83618409104f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(9922, 200)\n",
       "  (decoder): Linear(in_features=200, out_features=9922, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размер словаря\n",
    "vocab_size = vocab.__len__()\n",
    "# размер векторных представлений\n",
    "emb_size = 200\n",
    "# размерность скрытого полносвязного слоя в nn.TransformerEncoder\n",
    "hidden_dim = 200 \n",
    "# число  nn.TransformerEncoderLayer слоев в nn.TransformerEncoder\n",
    "n_blocks = 2\n",
    "# число голов  в  multiheadattention \n",
    "n_heads = 2\n",
    "# величина dropout\n",
    "dropout = 0.2\n",
    "\n",
    "# инициализируем нашу модель класса TransformerModel,\n",
    "# передав в качестве аргументов:\n",
    "# размер словаря\n",
    "# размер векторных представлений\n",
    "# число голов  в  multiheadattention \n",
    "# размерность скрытого полносвязного слоя в nn.TransformerEncoder\n",
    "# число  nn.TransformerEncoderLayer слоев в nn.TransformerEncoder\n",
    "# величину dropout\n",
    "model = TransformerModel(vocab_size, emb_size, n_heads, hidden_dim, n_blocks, dropout)\n",
    "# перенесем модель на девайс\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0oh07HXQHtb"
   },
   "source": [
    "# Обучение модели\n",
    "\n",
    "\n",
    "В качестве лосса используем CrossEntropyLoss, в качестве оптимайзера -- Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "W84myrlJQHtf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "LOG_INTERVAL = 500\n",
    "EPOCHS = 3 # epoch\n",
    "LR = 5  # learning rate - lr\n",
    "# определим функцию потерь torch.nn.CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# определим оптимизатор torch.optim.SGD с заданным lr, \n",
    "# передав также параметры модели model.parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# определим расписание изменения значения lr torch.optim.lr_scheduler.StepLR\n",
    "# передав в качестве аргументов optimizer, \n",
    "# значение step_size равное 1, и значение gamma 0.95 (коэффициент убывания)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # создадим переменную для сохранения лоссов на обучающей выборке\n",
    "    train_losses = []\n",
    "    # обязательно переводим модель в режим обучения с помощью метода train()\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # используем метод модели generate_square_subsequent_mask для генерации маски,\n",
    "    # передавая в качестве аргумента bptt, а также перенесем результат на девайс\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    # итерируемся батчами по заданному датасету\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        # получаем данные для батча под номером i\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # обнуляем градиенты с помощью метода zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if data.size(0) != bptt:\n",
    "            # используем метод модели generate_square_subsequent_mask для генерации маски,\n",
    "            # передавая в качестве аргумента bptt, а также перенесем результат на девайс\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            \n",
    "        # передаем в модель data и src_mask\n",
    "        output = model(data, src_mask)\n",
    "        # получим распределение logits по выходным токенам \n",
    "        output_tokens = output.view(-1, vocab_size)\n",
    "        # подсчитываем лосс с помощью criterion, \n",
    "        # вычисляемого на основе output_tokens и targets \n",
    "        # (напоминаю, targets -- это те же токены, сдвинутые на 1 вперед)\n",
    "        loss = criterion(output_tokens, targets)\n",
    "        # обратное распространение ошибки с помощью метода backward()\n",
    "        loss.backward()\n",
    "        # ограничиваем норму градиентов с помощью метода \n",
    "        # torch.nn.utils.clip_grad_norm_, в который передаются \n",
    "        # параметры модели model.parameters() и макс. значение нормы 0.5\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        # делаем шаг оптимизатора, то есть обновляем веса модели, \n",
    "        # с помощью метода step()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # добавим текущее значение лосса на батче обучающей выборки в наш лист\n",
    "        # train_losses, который потом будем использовать для печати лоссов\n",
    "        train_losses += [loss.item()]\n",
    "        \n",
    "        # теперь проверяем, не настоли ли время логгирования скоров на обучающей выборке\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / LOG_INTERVAL,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    # обязательно переводим модель в режим инференса (эвалюации) \n",
    "    # с помощью метода eval()\n",
    "    eval_model.eval() \n",
    "    \n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    # эвалюация не должна обновлять градиенты модели\n",
    "    with torch.no_grad():\n",
    "          # итерируемся батчами по заданному датасету\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            # получаем данные для батча под номером i\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                # используем метод модели generate_square_subsequent_mask для генерации маски,\n",
    "                # передавая в качестве аргумента bptt, а также перенесем результат на девайс\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            # передаем в модель data и src_mask\n",
    "            output = model(data, src_mask)\n",
    "            # получим распределение logits по выходным токенам \n",
    "            output_flat = output.view(-1, vocab_size)\n",
    "            # подсчитываем лосс с помощью criterion, \n",
    "            # вычисляемого на основе output_tokens и targets \n",
    "            # (напоминаю, targets -- это те же токены, сдвинутые на 1 вперед)\n",
    "            valid_loss = criterion(output_flat, targets).item()\n",
    "            total_loss += len(data) * valid_loss\n",
    "    \n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BPMW4m9QHti"
   },
   "source": [
    "Запускаем тренировку и смотрим, как падает лосс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYLlzbstQHtm",
    "outputId": "b18dd091-7f24-4060-e456-d8f13cbffd6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1320 batches | lr 5.00 | ms/batch 138.35 | loss  6.53 | ppl   688.35\n",
      "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch 135.41 | loss  5.69 | ppl   295.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 190.92s | valid loss  5.50 | valid ppl   244.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   500/ 1320 batches | lr 4.75 | ms/batch 132.09 | loss  5.39 | ppl   219.88\n",
      "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch 133.02 | loss  5.28 | ppl   197.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 183.25s | valid loss  5.32 | valid ppl   204.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   500/ 1320 batches | lr 4.51 | ms/batch 153.69 | loss  5.17 | ppl   175.28\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "# зададим переменные для сохранения хначения лоссов\n",
    "# для дальнейшего построения графиков\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# подсчитаем значения лосса на валидационной выборке до начала обучения с помощью нашей функции evaluate\n",
    "val_loss = evaluate(model, val_data)\n",
    "# сохраним первое значение лосса на валидационной подвыборке\n",
    "eval_losses += [val_loss]\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    # запускаем обучения на 1 эпоху с помощью нашей функции train\n",
    "    train_losses += train()\n",
    "    # подсчитываем качество на валидационной подвыборке \n",
    "    # с помощью нашей функции evaluate\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    eval_losses += [val_loss]\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoWkFlbfQHtn",
    "outputId": "c5b0a572-9f14-428a-9533-2228a84b6f34"
   },
   "outputs": [],
   "source": [
    "len(train_losses), len(eval_losses), len(train_losses) / EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usOj4Bg_QHto"
   },
   "source": [
    "## Построим графики loss значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "-hTC6UfbQHto",
    "outputId": "37700e39-57f6-4078-d7f2-70952d82224a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#%% matplotlib.inline\n",
    "\n",
    "\n",
    "# так как в train_losses хранятся лоссы с каждого батча, то их слишком много \n",
    "# для качественной и наглядной визуализации. \n",
    "# Построим также сглаженное значение train_losses c заданным SMOOTHING_PERIOD\n",
    "SMOOTHING_PERIOD = 500\n",
    "\n",
    "plt.figure(figsize=[12, 6])\n",
    "# строим полученные лоссы с каждого батча обучения\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "\n",
    "# получим сглаженные значения train_losses\n",
    "smoothed = np.convolve(train_losses, np.ones(SMOOTHING_PERIOD)/SMOOTHING_PERIOD)\n",
    "# строим сглаженные лоссы обучения\n",
    "plt.plot(smoothed[SMOOTHING_PERIOD:-SMOOTHING_PERIOD], label=\"train-smoothed\")\n",
    "\n",
    "# строим полученные лоссы с каждой эвалюации\n",
    "plt.plot(len(train_losses) / EPOCHS * np.arange(EPOCHS + 1), eval_losses, label=\"valid\")\n",
    "\n",
    "# добавим легенду для интерпретируемости\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
